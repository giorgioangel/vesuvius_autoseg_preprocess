{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import load_tifstack, free_memory, chunk_generator\n",
    "from skimage.restoration import estimate_sigma\n",
    "from cucim.skimage.exposure import equalize_adapthist\n",
    "from fast_nl_means import nlm_3d\n",
    "import cupy as cp\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from multiprocessing import Manager, Process, Queue, Value, Lock\n",
    "import blosc2\n",
    "import os\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scroll = load_tifstack(\"../scroll1/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape = scroll.shape\n",
    "chunk_size = [800, 800, 800]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_gpus = 8\n",
    "# Create a queue for each GPU using Queue for multiprocessing\n",
    "gpu_queues = [Queue() for _ in range(num_gpus)]\n",
    "manager = Manager()\n",
    "return_dict = manager.dict()\n",
    "lock = Lock()\n",
    "total_chunks = Value('i', 0)\n",
    "processed_chunks = Value('i', 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Producer function to assign chunks to GPU queues dynamically\n",
    "def producer(scroll, chunk_size, gpu_queues, total_chunks):\n",
    "    for i, (z, y, x) in tqdm(enumerate(chunk_generator(scroll.shape, chunk_size))):\n",
    "        gpu_id = i % num_gpus\n",
    "        chunk_id = (z, y, x)\n",
    "        chunk = scroll[z:z+chunk_size[0], y:y+chunk_size[1], x:x+chunk_size[2]].astype(np.float64)\n",
    "        gpu_queues[gpu_id].put((chunk_id, chunk))\n",
    "        total_chunks.value += 1\n",
    "        delta = total_chunks.value - processed_chunks.value\n",
    "        sleep(20*(delta//8))\n",
    "    # Signal the end of the data with a special value (None)\n",
    "    for gpu_queue in gpu_queues:\n",
    "        gpu_queue.put(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consumer function to process chunks on GPU\n",
    "def process_chunk_on_gpu(gpu_id, task_queue, return_dict, processed_chunk, lock):\n",
    "    cp.cuda.Device(gpu_id).use()\n",
    "    while True:\n",
    "        item = task_queue.get()\n",
    "        if item is None:\n",
    "            break\n",
    "        chunk_id, chunk = item\n",
    "        cp.get_default_memory_pool().free_all_blocks()\n",
    "        cp.get_default_pinned_memory_pool().free_all_blocks()\n",
    "        chunk /= 65535.\n",
    "        sigma = estimate_sigma(chunk)\n",
    "        chunk = cp.array(chunk)\n",
    "        chunk = nlm_3d(chunk, patch_size=7, patch_distance=3, sigma=sigma, h=0.03)\n",
    "        free_memory()\n",
    "        cp.cuda.Stream.null.synchronize()\n",
    "\n",
    "        #normalizing\n",
    "        min_chunk = cp.min(chunk)\n",
    "        if min_chunk < 0:\n",
    "            chunk -= min_chunk\n",
    "        max_chunk = cp.max(chunk)\n",
    "        if max_chunk > 1:\n",
    "            chunk /= max_chunk\n",
    "        \n",
    "        chunk = equalize_adapthist(chunk, kernel_size=8, clip_limit=0.01, nbins=256)\n",
    "\n",
    "        #normalizing\n",
    "        min_chunk = cp.min(chunk)\n",
    "        if min_chunk < 0:\n",
    "            chunk -= cp.min(chunk)\n",
    "        max_chunk = cp.max(chunk)\n",
    "        if max_chunk > 1:\n",
    "            chunk /= max_chunk\n",
    "\n",
    "        chunk *= 65535\n",
    "        chunk = cp.clip(chunk, 0, 65535) # just in case...\n",
    "        \n",
    "        temp_chunk = (chunk.get()).astype(np.uint16)\n",
    "        with lock:\n",
    "            return_dict[chunk_id] = temp_chunk\n",
    "            processed_chunk.value += 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writer_process(output_folder, chunk_size, return_dict, total_chunks, processed_chunks):\n",
    "    clevel = 9\n",
    "    nthreads = 200\n",
    "    cparams = {\n",
    "            \"codec\": blosc2.Codec.ZSTD,\n",
    "            \"clevel\": clevel,\n",
    "            \"filters\": [blosc2.Filter.BITSHUFFLE, blosc2.Filter.BYTEDELTA],\n",
    "            \"filters_meta\": [0, 0],\n",
    "            \"nthreads\": nthreads,\n",
    "    }\n",
    "    \n",
    "    while True:\n",
    "        if processed_chunks.value == total_chunks.value and len(return_dict) == 0:\n",
    "            break\n",
    "        for chunk_id, denoised_chunk in list(return_dict.items()):\n",
    "            z, y, x = chunk_id\n",
    "            filepath = os.path.join(output_folder, f\"chunk_z_y_x_{z}_{y}_{x}.b2nd\")\n",
    "            try:\n",
    "                denoised_array = blosc2.empty(denoised_chunk.shape, dtype=np.uint16, chunks=(chunk_size[0],chunk_size[1],chunk_size[2]), blocks=(100,100,100), urlpath=filepath, cparams=cparams)\n",
    "                denoised_array[:,:,:] = denoised_chunk\n",
    "            except:\n",
    "                continue\n",
    "            del return_dict[chunk_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and start a producer process\n",
    "producer_process = Process(target=producer, args=(scroll, chunk_size, gpu_queues, total_chunks))\n",
    "producer_process.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and start a process for each GPU\n",
    "processes = []\n",
    "for gpu_id in range(num_gpus):\n",
    "    p = Process(target=process_chunk_on_gpu, args=(gpu_id, gpu_queues[gpu_id], return_dict, processed_chunks, lock))\n",
    "    processes.append(p)\n",
    "    p.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and start the writer process\n",
    "writer = Process(target=writer_process, args=(\"./scroll1-denoised\", chunk_size, return_dict, total_chunks, processed_chunks))\n",
    "writer.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait for the producer process to finish\n",
    "producer_process.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in processes:\n",
    "    p.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in processes:\n",
    "    p.join()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
